# =============================================================================
# LIMIT STORAGE SIZES - COST CONTROL POLICY
# =============================================================================
# Controls storage sizes across different storage services to manage costs

import "tfplan/v2" as tfplan
import "strings"

# Storage limits by environment (in GB)
storage_limits = {
  "dev": {
    "ebs_volume": 100,
    "ebs_total": 500,
    "s3_lifecycle_days": 30,
    "efs_throughput": "provisioned",
    "efs_max_throughput": 100, # MiB/s
  },
  "staging": {
    "ebs_volume": 500,
    "ebs_total": 2000,
    "s3_lifecycle_days": 90,
    "efs_throughput": "provisioned",
    "efs_max_throughput": 500,
  },
  "prod": {
    "ebs_volume": 1000,
    "ebs_total": 10000,
    "s3_lifecycle_days": 365,
    "efs_throughput": "provisioned",
    "efs_max_throughput": 1000,
  },
}

# Get all storage-related resources
ebs_volumes = filter tfplan.resource_changes as _, resource_changes {
  resource_changes.type is "aws_ebs_volume" and
  resource_changes.mode is "managed" and
  (resource_changes.change.actions contains "create" or
   resource_changes.change.actions contains "update")
}

s3_buckets = filter tfplan.resource_changes as _, resource_changes {
  resource_changes.type is "aws_s3_bucket" and
  resource_changes.mode is "managed" and
  (resource_changes.change.actions contains "create" or
   resource_changes.change.actions contains "update")
}

s3_lifecycle_configs = filter tfplan.resource_changes as _, resource_changes {
  resource_changes.type is "aws_s3_bucket_lifecycle_configuration" and
  resource_changes.mode is "managed" and
  (resource_changes.change.actions contains "create" or
   resource_changes.change.actions contains "update")
}

efs_file_systems = filter tfplan.resource_changes as _, resource_changes {
  resource_changes.type is "aws_efs_file_system" and
  resource_changes.mode is "managed" and
  (resource_changes.change.actions contains "create" or
   resource_changes.change.actions contains "update")
}

# Function to determine environment
determine_environment = func(address, tags) {
  # Check address for environment indicators
  if strings.contains(strings.to_lower(address), "dev") {
    return "dev"
  } else if strings.contains(strings.to_lower(address), "staging") {
    return "staging"
  } else if strings.contains(strings.to_lower(address), "prod") {
    return "prod"
  }
  
  # Check tags for environment
  if tags is not null {
    if "Environment" in tags {
      env = strings.to_lower(tags.Environment else "")
      if strings.contains(env, "dev") {
        return "dev"
      } else if strings.contains(env, "staging") {
        return "staging"
      } else if strings.contains(env, "prod") {
        return "prod"
      }
    }
  }
  
  return "prod"  # Default to production limits
}

# Validate EBS volume sizes
validate_ebs_volumes = rule {
  all ebs_volumes as address, rc {
    size = rc.change.after.size else 0
    volume_type = rc.change.after.type else "gp2"
    tags = rc.change.after.tags else {}
    
    environment = determine_environment(address, tags)
    max_size = storage_limits[environment]["ebs_volume"]
    
    # Apply stricter limits for expensive volume types
    adjusted_max_size = max_size
    if volume_type is "io1" or volume_type is "io2" {
      adjusted_max_size = max_size / 2  # Halve the limit for IOPS volumes
    }
    
    size <= adjusted_max_size else {
      print("COST CONTROL VIOLATION:")
      print("EBS volume", address, "size", size, "GB exceeds limit")
      print("Volume type:", volume_type)
      print("Environment:", environment)
      print("Maximum allowed size:", adjusted_max_size, "GB")
      if volume_type is "io1" or volume_type is "io2" {
        print("Note: IOPS volumes have reduced size limits due to higher costs")
      }
      print("Consider using smaller volumes or multiple volumes if needed.")
      false
    }
  }
}

# Check total EBS storage per environment/region
validate_total_ebs_storage = rule {
  # Group volumes by environment
  dev_volumes = filter ebs_volumes as address, rc {
    tags = rc.change.after.tags else {}
    environment = determine_environment(address, tags)
    environment is "dev"
  }
  
  staging_volumes = filter ebs_volumes as address, rc {
    tags = rc.change.after.tags else {}
    environment = determine_environment(address, tags)
    environment is "staging"
  }
  
  prod_volumes = filter ebs_volumes as address, rc {
    tags = rc.change.after.tags else {}
    environment = determine_environment(address, tags)
    environment is "prod"
  }
  
  # Calculate totals
  dev_total = 0
  for dev_volumes as address, rc {
    dev_total += rc.change.after.size else 0
  }
  
  staging_total = 0
  for staging_volumes as address, rc {
    staging_total += rc.change.after.size else 0
  }
  
  prod_total = 0
  for prod_volumes as address, rc {
    prod_total += rc.change.after.size else 0
  }
  
  # Validate totals
  dev_valid = dev_total <= storage_limits["dev"]["ebs_total"]
  staging_valid = staging_total <= storage_limits["staging"]["ebs_total"]
  prod_valid = prod_total <= storage_limits["prod"]["ebs_total"]
  
  dev_valid and staging_valid and prod_valid else {
    if not dev_valid {
      print("COST CONTROL VIOLATION:")
      print("Total EBS storage for development exceeds limit")
      print("Current total:", dev_total, "GB")
      print("Maximum allowed:", storage_limits["dev"]["ebs_total"], "GB")
    }
    if not staging_valid {
      print("COST CONTROL VIOLATION:")
      print("Total EBS storage for staging exceeds limit")
      print("Current total:", staging_total, "GB")
      print("Maximum allowed:", storage_limits["staging"]["ebs_total"], "GB")
    }
    if not prod_valid {
      print("COST CONTROL VIOLATION:")
      print("Total EBS storage for production exceeds limit")
      print("Current total:", prod_total, "GB")
      print("Maximum allowed:", storage_limits["prod"]["ebs_total"], "GB")
    }
    false
  }
}

# Validate S3 lifecycle policies exist
validate_s3_lifecycle = rule {
  all s3_buckets as address, rc {
    tags = rc.change.after.tags else {}
    environment = determine_environment(address, tags)
    
    # Check if there's a corresponding lifecycle configuration
    bucket_name = rc.change.after.bucket else ""
    
    # For now, just recommend lifecycle policies
    print("COST CONTROL RECOMMENDATION:")
    print("S3 bucket", address, "should have lifecycle policies")
    print("Environment:", environment)
    print("Recommended lifecycle transition after", storage_limits[environment]["s3_lifecycle_days"], "days")
    print("This helps reduce storage costs by moving old data to cheaper storage classes.")
    
    true  # Advisory only
  }
}

# Validate EFS throughput settings
validate_efs_throughput = rule {
  all efs_file_systems as address, rc {
    throughput_mode = rc.change.after.throughput_mode else "bursting"
    provisioned_throughput = rc.change.after.provisioned_throughput_in_mibps else 0
    tags = rc.change.after.tags else {}
    
    environment = determine_environment(address, tags)
    max_throughput = storage_limits[environment]["efs_max_throughput"]
    
    # Check provisioned throughput limits
    if throughput_mode is "provisioned" {
      provisioned_throughput <= max_throughput else {
        print("COST CONTROL VIOLATION:")
        print("EFS file system", address, "provisioned throughput exceeds limit")
        print("Current throughput:", provisioned_throughput, "MiB/s")
        print("Environment:", environment)
        print("Maximum allowed:", max_throughput, "MiB/s")
        print("Higher throughput increases costs significantly.")
        false
      }
    } else {
      # Recommend bursting mode for non-production
      if environment is not "prod" {
        throughput_mode is "bursting" else {
          print("COST CONTROL WARNING:")
          print("EFS file system", address, "uses provisioned throughput in", environment)
          print("Consider using bursting mode for cost savings in non-production environments.")
          true  # Warning only
        }
      } else {
        true
      }
    }
  }
}

# Check for expensive storage classes
validate_storage_classes = rule {
  # This would require looking at S3 bucket configurations
  # For now, provide guidance
  length(s3_buckets) > 0 else true  # If no S3 buckets, pass
  
  # For each bucket, recommend appropriate storage classes
  all s3_buckets as address, rc {
    tags = rc.change.after.tags else {}
    environment = determine_environment(address, tags)
    
    print("COST CONTROL RECOMMENDATION:")
    print("S3 bucket", address, "in", environment, "environment")
    print("Consider using appropriate storage classes:")
    if environment is "dev" {
      print("- Standard for active data")
      print("- Standard-IA for infrequent access")
      print("- Glacier for archival (after", storage_limits[environment]["s3_lifecycle_days"], "days)")
    } else if environment is "staging" {
      print("- Standard for testing data")
      print("- Standard-IA for older test data")
      print("- Glacier for long-term test data")
    } else {
      print("- Standard for active production data")
      print("- Standard-IA for infrequent access")
      print("- Glacier/Deep Archive for compliance/archival")
    }
    
    true  # Advisory only
  }
}

# Main rule
main = rule {
  validate_ebs_volumes and
  validate_total_ebs_storage and
  validate_s3_lifecycle and
  validate_efs_throughput and
  validate_storage_classes
}
